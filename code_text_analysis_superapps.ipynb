{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lQcSLUOtdFN"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import nltk\n",
    "from tabulate import tabulate\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOwhWcRhu9PP"
   },
   "source": [
    "## Loading data from Twitter (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV files and concatenating them into a single DataFrame\n",
    "data_twitter = pd.concat([\n",
    "    pd.read_csv('filtered_super_app_2.csv'),\n",
    "    pd.read_csv('filtered_super_apps_tweets.csv')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Adding a new column called 'source' with the value 'super_apps_twitter'\n",
    "data_twitter['source'] = 'super_apps_twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_twitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting the language of the tweets and filtering out non-English tweets\n",
    "import langid\n",
    "data_twitter[\"language_det\"] = data_twitter[\"Tweet_Content\"].apply(lambda x: langid.classify(x)[0] if isinstance(x, str) else \"\")\n",
    "data_twitter = data_twitter[data_twitter[\"language_det\"] == \"en\"]\n",
    "print(len(data_twitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the superapp phrase is in the username, thats why we are filtering tweets that contain the phrase in the tweet content\n",
    "# Defining the phrases to search for\n",
    "phrases = [\"super app\", \"superapp\", \"super-app\", \"super apps\", \"superapps\", \"super-apps\"]\n",
    "\n",
    "# Creating a regex pattern to match any of the phrases (case-insensitive)\n",
    "pattern = '|'.join(map(re.escape, phrases))\n",
    "\n",
    "# Filtering the dataset for tweets containing any of the phrases\n",
    "data_twitter = data_twitter[data_twitter['Tweet_Content'].str.contains(pattern, case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to categorize data based on conditions\n",
    "def categorize(row):\n",
    "    # Converting comments to lowercase for case-insensitive matching\n",
    "    text = row['Tweet_Content'].lower()\n",
    "    if 'wechat' in text:\n",
    "        return 'wechat'\n",
    "    elif 'kakaotalk' in text or 'kakao talk' in text or 'kakao-talk' in text:\n",
    "        return 'kakaotalk'\n",
    "    elif 'gojek' in text:\n",
    "        return 'gojek'\n",
    "    elif 'grab' in text:\n",
    "        return 'grab'\n",
    "    elif 'revolut' in text:\n",
    "        return 'revolut'\n",
    "    elif 'alipay' in text:\n",
    "        return 'alipay'\n",
    "    else:\n",
    "        return 'other'  \n",
    "\n",
    "# Applying the function to create the new 'category' column\n",
    "data_twitter['category'] = data_twitter.apply(categorize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_youtube_1 = [\n",
    "    pd.read_csv('YouTube Why Its so hard to.csv'),\n",
    "    pd.read_csv('YouTube CNBC.csv'),\n",
    "    pd.read_csv('YouTube Elon_failed.csv'),\n",
    "    pd.read_csv('YouTube Why_elon.csv'),\n",
    "    pd.read_csv('YouTube CNBC_what.csv'),\n",
    "    pd.read_csv('YouTube How_china_is_changing.csv')\n",
    "    ]\n",
    "dfs_youtube_1 = pd.concat(dfs_youtube_1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_revolut_1 = [\n",
    "    pd.read_csv('YouTube truth_revolut.csv'),\n",
    "    pd.read_csv('YouTube revolut_vs_wise.csv'),\n",
    "]\n",
    "dfs_grab_1 = [\n",
    "    pd.read_csv('YouTube grab.csv'),\n",
    "    pd.read_csv('YouTube revolut_+_grab.csv')\n",
    "]\n",
    "dfs_kakaotalk_1 = [\n",
    "    pd.read_csv('YouTube KakaoTalk_how_to_find.csv'),\n",
    "    pd.read_csv('YouTube KakaoTalk_signup.csv')\n",
    "]\n",
    "dfs_wechat_1 = [pd.read_csv('YouTube wechat_tencent.csv')]\n",
    "dfs_alipay_1 = [\n",
    "    pd.read_csv('YouTube alipay.csv'),\n",
    "    pd.read_csv('YouTube alipay_2.csv')\n",
    "]\n",
    "dfs_gojek_1 = [\n",
    "    pd.read_csv('YouTube gojek.csv')\n",
    "]\n",
    "dfs_gojek_1 = pd.concat(dfs_gojek_1, ignore_index=True)\n",
    "dfs_wechat_1 = pd.concat(dfs_wechat_1, ignore_index=True)\n",
    "dfs_alipay_1 = pd.concat(dfs_alipay_1, ignore_index=True)\n",
    "dfs_kakaotalk_1 = pd.concat(dfs_kakaotalk_1, ignore_index=True)\n",
    "dfs_grab_1 = pd.concat(dfs_grab_1, ignore_index=True)\n",
    "dfs_revolut_1 = pd.concat(dfs_revolut_1, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column called 'source' with certain value (for example: 'youtube_gojek') for all DataFrames in certain list (for example: 'dfs_gojek_1')\n",
    "dfs_gojek_1['category'] = 'gojek'\n",
    "dfs_wechat_1['category'] = 'wechat'\n",
    "dfs_alipay_1['category'] = 'alipay'\n",
    "dfs_kakaotalk_1['category'] = 'kakaotalk'\n",
    "dfs_grab_1['category'] = 'grab'\n",
    "dfs_revolut_1['category'] = 'revolut'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to categorize data based on conditions\n",
    "def categorize(row):\n",
    "    # Converting the comment to lowercase for case-insensitive matching\n",
    "    text = row['Comment'].lower()\n",
    "    if 'wechat' in text:\n",
    "        return 'wechat'\n",
    "    elif 'kakaotalk' in text or 'kakao talk' in text or 'kakao-talk' in text:\n",
    "        return 'kakaotalk'\n",
    "    elif 'gojek' in text:\n",
    "        return 'gojek'\n",
    "    elif 'grab' in text:\n",
    "        return 'grab'\n",
    "    elif 'revolut' in text:\n",
    "        return 'revolut'\n",
    "    elif 'alipay' in text:\n",
    "        return 'alipay'\n",
    "    else:\n",
    "        return 'other' \n",
    "\n",
    "# Applying the function to create the new 'category' column\n",
    "dfs_youtube_1['category'] = dfs_youtube_1.apply(categorize, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging youtube data\n",
    "data_youtube_1 = pd.concat([dfs_youtube_1, dfs_alipay_1, dfs_gojek_1, dfs_grab_1, dfs_kakaotalk_1, dfs_revolut_1, dfs_wechat_1], ignore_index=True)\n",
    "data_youtube_1['source'] = 'super_apps_youtube'\n",
    "data_youtube_1.head()\n",
    "print(len(data_youtube_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting the language of the tweets and filtering out non-English tweets\n",
    "data_youtube_1[\"language_det\"] = data_youtube_1[\"Comment\"].apply(lambda x: langid.classify(x)[0] if isinstance(x, str) else \"\")\n",
    "data_youtube_1 = data_youtube_1[data_youtube_1[\"language_det\"] == \"en\"]\n",
    "print(len(data_youtube_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRP2EIUysXtR"
   },
   "source": [
    "# Data preprocessing and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column dropping + duplicates checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unwanted columns and renaming columns - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying current columns first\n",
    "print(\"Original columns in super_app:\")\n",
    "print(data_twitter.columns.tolist())\n",
    "\n",
    "\n",
    "# Renaming specific columns\n",
    "renamed_columns = {\n",
    "    'UTC_Time': 'Date_of_comment',\n",
    "    'Tweet_Content': 'Comment',\n",
    "}\n",
    "\n",
    "data_twitter = data_twitter.rename(columns=renamed_columns)\n",
    "\n",
    "# Defining columns to remove (adjust based on actual column names)\n",
    "columns_to_drop = [\n",
    "    'Author_Web_Page_URL', \n",
    "    'Post_ID', \n",
    "    'Tweet_URL',\n",
    "    'Language',\n",
    "    'Replying_to',\n",
    "    'Replying_to Whom',\n",
    "    'Replying_to_Whom_URL',\n",
    "    'Replying_to_Whom_Username',\n",
    "    'Reply_to_Whom_Handle',\n",
    "    'Replying_to',\n",
    "    'Tweet_image_URL',\n",
    "    'Bookmark_Count',\n",
    "    'View_Count',\n",
    "    'Reply_Count',\n",
    "    'Repost_Count',\n",
    "    'Ads',\n",
    "    'Verified',\n",
    "    'Author_Handle',\n",
    "    'Post_URL',\n",
    "    'Query_Str',\n",
    "    'Verified_Status',\n",
    "    'Like_Count',\n",
    "    'Tweet_Image_URL',\n",
    "    'Reply_to_Whom',\n",
    "    'Reply_to_Whom_URL',\n",
    "    'Reply_to_Whom_Username',\n",
    "    'language_det'\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Safe removal - only drop existing columns\n",
    "def safe_column_drop(df, columns):\n",
    "    existing_cols = [col for col in columns if col in df.columns]\n",
    "    return df.drop(existing_cols, axis=1)\n",
    "\n",
    "# Applying the function to dataset\n",
    "data_twitter = safe_column_drop(data_twitter, columns_to_drop)\n",
    "\n",
    "\n",
    "# Verifying remaining columns\n",
    "print(\"\\nColumns after removal in super_app:\")\n",
    "print(data_twitter.columns.tolist())\n",
    "\n",
    "\n",
    "data_twitter.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting rid of duplicates in Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    # Printing the original row count\n",
    "    print(f\"Original row count: {len(df)}\")\n",
    "    \n",
    "    # Removing duplicates based on 'Tweet_Content'\n",
    "    df_unique = df.drop_duplicates(subset='Comment', keep='first')\n",
    "    \n",
    "    # Printing new row count\n",
    "    print(f\"Row count after removing duplicates: {len(df_unique)}\")\n",
    "    \n",
    "    # Calculating the number of duplicates removed\n",
    "    duplicates_removed = len(df) - len(df_unique)\n",
    "    print(f\"Number of duplicates removed: {duplicates_removed}\\n\")\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "# Processing the merged DataFrame\n",
    "print(\"Processing merged dataset:\")\n",
    "data_twitter = remove_duplicates(data_twitter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unwanted columns and renaming columns - Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming specific columns\n",
    "renamed_columns = {\n",
    "    'Video Title': 'Title_of_video',\n",
    "    'Author': 'Author_Name',\n",
    "    'Date and Time': 'Date_of_comment',\n",
    "    'Comment': 'Comment'\n",
    "}\n",
    "\n",
    "data_youtube_1 = data_youtube_1.rename(columns=renamed_columns)\n",
    "data_youtube_1 = data_youtube_1.drop(columns=['Title_of_video', 'language_det'])\n",
    "\n",
    "\n",
    "data_youtube_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting rid of duplicates in YouTube data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    # Printing the original row count\n",
    "    print(f\"Original row count: {len(df)}\")\n",
    "    \n",
    "    # Removing duplicates based on 'Tweet_Content'\n",
    "    df_unique = df.drop_duplicates(subset='Comment', keep='first')\n",
    "    \n",
    "    # Printing new row count\n",
    "    print(f\"Row count after removing duplicates: {len(df_unique)}\")\n",
    "    \n",
    "    # Calculating the number of duplicates removed\n",
    "    duplicates_removed = len(df) - len(df_unique)\n",
    "    print(f\"Number of duplicates removed: {duplicates_removed}\\n\")\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "# Processing the merged DataFrame\n",
    "print(\"Processing merged dataset:\")\n",
    "data_youtube_1 = remove_duplicates(data_youtube_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting time to UTC format + editing the \"Date_of_comment\" column - YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import dateparser\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "\n",
    "# Function to clean and convert dates\n",
    "def convert_date(date_str):\n",
    "    # Remove unwanted phrases\n",
    "    date_str = date_str.replace(\"(upraveno)\", \"\").replace(\"(edited)\", \"\").strip()\n",
    "    \n",
    "    # Convert date string to datetime object\n",
    "    try:\n",
    "        dt = dateparser.parse(date_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date: {date_str}. Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if dt is None:\n",
    "        # If dateparser fails, try manual conversion for \"X months ago\"\n",
    "        if \"months ago\" in date_str:\n",
    "            months_ago = int(date_str.split(\"months ago\")[0])\n",
    "            dt = datetime.now() - timedelta(days=months_ago*30)\n",
    "        elif \"months\" in date_str:\n",
    "            months_ago = int(date_str.split(\"months\")[0])\n",
    "            dt = datetime.now() - timedelta(days=months_ago*30)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Set date to the 15th of the month\n",
    "    dt = dt.replace(day=15, hour=12, minute=0, second=0)\n",
    "    \n",
    "    # Convert to UTC timezone\n",
    "    dt = dt.astimezone(tz=None)  # Assuming system timezone is UTC\n",
    "    \n",
    "    return dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Apply the function to the 'comment_time' column\n",
    "data_youtube_1['Date_of_comment'] = data_youtube_1['Date_of_comment'].apply(convert_date)\n",
    "data_youtube_1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data from Twitter and Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Aligning columns\n",
    "all_columns = set(data_twitter.columns) | set(data_youtube_1.columns)\n",
    "data_twitter = data_twitter.reindex(columns=all_columns)\n",
    "data_youtube_1 = data_youtube_1.reindex(columns=all_columns)\n",
    "\n",
    "# Concatenating DataFrames\n",
    "data = pd.concat([data_twitter, data_youtube_1], ignore_index=True)\n",
    "\n",
    "# Saving the merged DataFrame to a new CSV file\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicates in merged file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    # Printing original row count\n",
    "    print(f\"Original row count: {len(df)}\")\n",
    "    \n",
    "    # Removing duplicates based on 'Tweet_Content'\n",
    "    df_unique = df.drop_duplicates(subset='Comment', keep='first')\n",
    "    \n",
    "    # Printing the new row count\n",
    "    print(f\"Row count after removing duplicates: {len(df_unique)}\")\n",
    "    \n",
    "    # Calculating the number of duplicates removed\n",
    "    duplicates_removed = len(df) - len(df_unique)\n",
    "    print(f\"Number of duplicates removed: {duplicates_removed}\\n\")\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "# Processing the merged DataFrame\n",
    "print(\"Processing merged dataset:\")\n",
    "clean_data = remove_duplicates(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking and deleting missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the row(s) if they are not important\n",
    "data = data[data['Comment'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Checking for missing values\n",
    "print(data['Comment'].isnull().sum())\n",
    "\n",
    "# Checking for non-string values\n",
    "print(data['Comment'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the format of the Date_of_comment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all data in Date_of_comment is in the same format\n",
    "data['Date_of_comment'] = pd.to_datetime(data['Date_of_comment'], errors='coerce', utc=True)\n",
    "\n",
    "# Check for any rows where the conversion failed\n",
    "invalid_dates = data[data['Date_of_comment'].isnull()]\n",
    "if not invalid_dates.empty:\n",
    "    print(f\"Found {len(invalid_dates)} rows with invalid dates. These rows will be removed.\")\n",
    "    data = data.dropna(subset=['Date_of_comment'])\n",
    "\n",
    "# Confirm the format\n",
    "print(\"All dates are now in the same format.\")\n",
    "print(data['Date_of_comment'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACy1m_RP_Hw-"
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyM9aP_4n8nH"
   },
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting indexes of rows with only empty spaces in column body\n",
    "empty_rows = data[data['Comment'].str.strip() == ''].index\n",
    "print(empty_rows.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the \"Title_of_video\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning function\n",
    "def clean_title(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "data['Title_of_video'] = data['Title_of_video'].astype(str).apply(clean_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the \"Author_Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning function\n",
    "def clean_author(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "#    text = ' '.join([contractions.fix(word) for word in text.split()]) # Expand contractions\n",
    "#    text = text.lower()  # Convert to lowercase\n",
    "#    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "data['Author_Name'] = data['Author_Name'].astype(str).apply(clean_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the \"Comment\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = ' '.join([contractions.fix(word) for word in text.split()]) # Expand contractions\n",
    "    text = text.lower()  # Convert to lowercase        ???????????????\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Applying the text functions\n",
    "data['cleaned_body'] = data['Comment'].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS function - to map tags to worndet tags used in lemmatization\n",
    "def wordnet_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    elif nltk_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# To convert pos tags to wordnet tags\n",
    "def lemmatize(text):\n",
    "\n",
    "  \t# Tokenize the text first\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Tagging part\n",
    "    pos_text = nltk.pos_tag(tokens)\n",
    "    tagged_text = []\n",
    "    for word, tag in pos_text:\n",
    "        tagged_text.append((word, wordnet_tagger(tag)))\n",
    "\n",
    "    # Lemmatization part\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = []\n",
    "    for word, tag in tagged_text:\n",
    "        if tag is None:\n",
    "            lemmatized_text.append(word)\n",
    "        else:\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_text\n",
    "\n",
    "data['lemma'] = data['cleaned_body'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords removal\n",
    "custom_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "data['tokens'] = data['lemma'].apply(lambda x: [word for word in x if word not in custom_stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Gensim dictionary and corpus\n",
    "dictionary = Dictionary(data['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlZ_NnuU2YLs"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline of used tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Converting UTC_Time to datetime if not already\n",
    "data['Date_of_comment'] = pd.to_datetime(data['Date_of_comment'], utc=True)\n",
    "\n",
    "# Sorting the merged dataframe by UTC_Time\n",
    "data = data.sort_values('Date_of_comment')\n",
    "\n",
    "# Finding the earliest and latest dates for the entire dataset\n",
    "earliest_comment = data['Date_of_comment'].min()\n",
    "latest_comment = data['Date_of_comment'].max()\n",
    "\n",
    "# Finding the earliest and latest dates for Twitter\n",
    "twitter_data = data[data['source'] == 'super_apps_twitter']\n",
    "earliest_tweet = twitter_data['Date_of_comment'].min()\n",
    "latest_tweet = twitter_data['Date_of_comment'].max()\n",
    "\n",
    "# Finding the earliest and latest dates for YouTube\n",
    "youtube_data = data[data['source'] == 'super_apps_youtube']\n",
    "earliest_youtube_comment = youtube_data['Date_of_comment'].min()\n",
    "latest_youtube_comment = youtube_data['Date_of_comment'].max()\n",
    "\n",
    "# Creating events list\n",
    "events = ['Earliest tweet', 'Latest tweet', 'Earliest YouTube comment', 'Latest YouTube comment']\n",
    "dates = [earliest_tweet, latest_tweet, earliest_youtube_comment, latest_youtube_comment]\n",
    "\n",
    "colors = ['skyblue', 'salmon', 'lightgreen', 'orange']\n",
    "color_names = ['Sky Blue', 'Salmon', 'Light Green', 'Orange']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plotting timeline for all events\n",
    "plt.hlines(1, min(dates), max(dates), colors='lightgray', linestyles='dotted')\n",
    "\n",
    "for i, (date, color) in enumerate(zip(dates, colors)):\n",
    "    plt.scatter(date, 1, color=color, zorder=5, s=100)\n",
    "\n",
    "plt.yticks([])  # Removing y-axis\n",
    "plt.xlabel('Timeline')\n",
    "plt.title('Timeline of \"Super App(s)\" Comments')\n",
    "\n",
    "# Formatting dates for the table\n",
    "formatted_dates = [date.strftime('%Y-%m-%d %H:%M:%S') for date in dates]\n",
    "\n",
    "table_data = [[events[i], formatted_dates[i], color_names[i]] for i in range(len(events))]\n",
    "column_labels = ['Event', 'Date', 'Color']\n",
    "plt.table(cellText=table_data, colLabels=column_labels, loc='bottom', cellLoc='center', bbox=[0.1, -0.5, 0.8, 0.3])\n",
    "\n",
    "plt.subplots_adjust(bottom=0.45)  # Leaving space for the table\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing exact dates and additional information\n",
    "print(\"Exact dates:\")\n",
    "for event, date in zip(events, dates):\n",
    "    print(f\"{event}: {date}\")\n",
    "\n",
    "print(f\"\\nTotal number of comments: {len(data)}\")\n",
    "print(f\"Number of 'super_apps' tweets: {len(twitter_data)}\")\n",
    "print(f\"Number of 'super_apps' YouTube comments: {len(youtube_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "# Ensuring 'Date_of_comment' is in datetime format\n",
    "data['Date_of_comment'] = pd.to_datetime(data['Date_of_comment'])\n",
    "\n",
    "# Separating data by source\n",
    "twitter_data = data[data['source'] == 'super_apps_twitter']\n",
    "youtube_data = data[data['source'] == 'super_apps_youtube']\n",
    "\n",
    "# Setting indices for both datasets\n",
    "twitter_data = twitter_data.set_index('Date_of_comment', drop=False)\n",
    "youtube_data = youtube_data.set_index('Date_of_comment', drop=False)\n",
    "\n",
    "# Getting current date in UTC\n",
    "current_date = datetime.datetime.now(pytz.utc)\n",
    "\n",
    "# Truncation of both datasets\n",
    "last_date = min(current_date, data['Date_of_comment'].max())\n",
    "twitter_data = twitter_data[twitter_data.index <= last_date]\n",
    "youtube_data = youtube_data[youtube_data.index <= last_date]\n",
    "\n",
    "# Resampling frequencies for both sources\n",
    "frequency_data = {\n",
    "    'daily': {\n",
    "        'twitter': twitter_data.resample('D').size(),\n",
    "        'youtube': youtube_data.resample('D').size()\n",
    "    },\n",
    "    'weekly': {\n",
    "        'twitter': twitter_data.resample('W').size(),\n",
    "        'youtube': youtube_data.resample('W').size()\n",
    "    },\n",
    "    'monthly': {\n",
    "        'twitter': twitter_data.resample('M').size(),\n",
    "        'youtube': youtube_data.resample('M').size()\n",
    "    },\n",
    "    'yearly': {\n",
    "        'twitter': twitter_data.resample('Y').size().fillna(0),\n",
    "        'youtube': youtube_data.resample('Y').size().fillna(0)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Creating side-by-side plots for each timeframe\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Daily frequencies - side by side\n",
    "plt.subplot(4, 2, 1)\n",
    "sns.lineplot(x=frequency_data['daily']['twitter'].index, y=frequency_data['daily']['twitter'].values, color='blue')\n",
    "plt.title('Daily Twitter Activity')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "sns.lineplot(x=frequency_data['daily']['youtube'].index, y=frequency_data['daily']['youtube'].values, color='orange')\n",
    "plt.title('Daily YouTube Activity')\n",
    "plt.ylabel('Number of Comments')\n",
    "\n",
    "# Weekly frequencies - side by side\n",
    "plt.subplot(4, 2, 3)\n",
    "sns.lineplot(x=frequency_data['weekly']['twitter'].index, y=frequency_data['weekly']['twitter'].values, color='blue')\n",
    "plt.title('Weekly Twitter Activity')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "sns.lineplot(x=frequency_data['weekly']['youtube'].index, y=frequency_data['weekly']['youtube'].values, color='orange')\n",
    "plt.title('Weekly YouTube Activity')\n",
    "plt.ylabel('Number of Comments')\n",
    "\n",
    "# Monthly frequencies - side by side\n",
    "plt.subplot(4, 2, 5)\n",
    "sns.lineplot(x=frequency_data['monthly']['twitter'].index, y=frequency_data['monthly']['twitter'].values, color='blue')\n",
    "plt.title('Monthly Twitter Activity')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "sns.lineplot(x=frequency_data['monthly']['youtube'].index, y=frequency_data['monthly']['youtube'].values, color='orange')\n",
    "plt.title('Monthly YouTube Activity')\n",
    "plt.ylabel('Number of Comments')\n",
    "\n",
    "# Yearly frequencies - side by side\n",
    "plt.subplot(4, 2, 7)\n",
    "# Converting indexes to year strings for twitter (X)\n",
    "twitter_yearly = frequency_data['yearly']['twitter']\n",
    "twitter_yearly.index = twitter_yearly.index.year.astype(str)\n",
    "sns.lineplot(x=twitter_yearly.index, y=twitter_yearly.values, color='blue')\n",
    "plt.title('Yearly Twitter Activity')\n",
    "plt.ylabel('Number of Tweets')\n",
    "\n",
    "plt.subplot(4, 2, 8)\n",
    "# Converting indexes to year strings for youtube\n",
    "youtube_yearly = frequency_data['yearly']['youtube']\n",
    "youtube_yearly.index = youtube_yearly.index.year.astype(str)\n",
    "sns.lineplot(x=youtube_yearly.index, y=youtube_yearly.values, color='orange')\n",
    "plt.title('Yearly YouTube Activity')\n",
    "plt.ylabel('Number of Comments')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing statistics\n",
    "print(f\"Total number of comments/tweets: {len(data)}\")\n",
    "print(f\"Twitter posts: {len(twitter_data)}\")\n",
    "print(f\"YouTube comments: {len(youtube_data)}\")\n",
    "print(f\"\\nDate range: from {data.index.min()} to {data.index.max()}\")\n",
    "print(f\"Twitter date range: {twitter_data.index.min()} to {twitter_data.index.max()}\")\n",
    "print(f\"YouTube date range: {youtube_data.index.min()} to {youtube_data.index.max()}\")\n",
    "\n",
    "# Printing max activity statistics by platform\n",
    "for platform in ['twitter', 'youtube']:\n",
    "    label = \"Tweets\" if platform == 'twitter' else \"Comments\"\n",
    "    print(f\"\\n{platform.capitalize()} Max Activity:\")\n",
    "    print(f\"Day: {frequency_data['daily'][platform].idxmax()} ({frequency_data['daily'][platform].max()} {label})\")\n",
    "    print(f\"Week: {frequency_data['weekly'][platform].idxmax()} ({frequency_data['weekly'][platform].max()} {label})\")\n",
    "    print(f\"Month: {frequency_data['monthly'][platform].idxmax().strftime('%Y-%m')} ({frequency_data['monthly'][platform].max()} {label})\")\n",
    "    print(f\"Year: {frequency_data['yearly'][platform].idxmax()} ({frequency_data['yearly'][platform].max()} {label})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Counting tweets per user\n",
    "user_tweet_counts = data['Author_Name'].value_counts()\n",
    "\n",
    "# Top 10 users by tweet count\n",
    "top_10_users = user_tweet_counts.head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_10_users.index, y=top_10_users.values)\n",
    "plt.title('Top 10 Users by Tweet/Comment Count')\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Number of Tweets/Comments')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing statistics\n",
    "print(f\"Total number of unique users: {len(user_tweet_counts)}\")\n",
    "print(f\"User with most tweets/comments: {user_tweet_counts.index[0]} ({user_tweet_counts.iloc[0]} tweets)\")\n",
    "print(\"\\nTop 10 users by tweet/comment count:\")\n",
    "print(top_10_users)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings so that we can see the comments in full size\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "pd.set_option('display.max_rows', None)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking users and their tweets/comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering comments by author\n",
    "life_platform_comments = data[data['Author_Name'] == 'Life Platform | Official']\n",
    "print(len(life_platform_comments))\n",
    "\n",
    "# Displaying comments\n",
    "print(life_platform_comments[['Date_of_comment', 'Comment']])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering comments by author\n",
    "iron_wil_becker_comments = data[data['Author_Name'] == 'Iron Wil Becker']\n",
    "print(len(iron_wil_becker_comments))\n",
    "\n",
    "# Displaying comments\n",
    "print(iron_wil_becker_comments[['Date_of_comment', 'Comment']])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all comments/tweets from certain users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out posts from the user \"Iron Wil Becker\"\n",
    "data = data[data['Author_Name'] != 'Iron Wil Becker']\n",
    "\n",
    "# Verifying the deletion\n",
    "print(f\"Remaining posts: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out posts from the user \"Life Platform | Official\"\n",
    "data = data[data['Author_Name'] != 'Life Platform | Official']\n",
    "\n",
    "# Verifying the deletion\n",
    "print(f\"Remaining posts: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned version of the User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Counting tweets per user\n",
    "user_tweet_counts = data['Author_Name'].value_counts()\n",
    "\n",
    "# Top 10 users by tweet count\n",
    "top_10_users = user_tweet_counts.head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_10_users.index, y=top_10_users.values)\n",
    "plt.title('Top 10 Users by Tweet/Comment Count')\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Number of Tweets/Comments')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing statistics\n",
    "print(f\"Total number of unique users: {len(user_tweet_counts)}\")\n",
    "print(f\"User with most tweets/comments: {user_tweet_counts.index[0]} ({user_tweet_counts.iloc[0]} tweets)\")\n",
    "print(\"\\nTop 10 users by tweet/comment count:\")\n",
    "print(top_10_users)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.findall(r'#(\\w+)', text.lower())\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Extracting hashtags from all tweets\n",
    "all_hashtags = data['Comment'].apply(extract_hashtags).sum()\n",
    "\n",
    "# Counting hashtag occurrences\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "\n",
    "# Removing 'superapp' and 'superapps' from the count\n",
    "del hashtag_counts['superapp']\n",
    "del hashtag_counts['superapps']\n",
    "\n",
    "# Top 20 hashtags\n",
    "top_20_hashtags = pd.DataFrame(hashtag_counts.most_common(20), columns=['Hashtag', 'Count'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Count', y='Hashtag', data=top_20_hashtags)\n",
    "plt.title('Top 20 Co-occurring Hashtags with \"super app(s)\"')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Hashtag')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing  statistics\n",
    "print(f\"Total number of hashtags used: {len(all_hashtags)}\")\n",
    "print(f\"Number of unique hashtags: {len(hashtag_counts)}\")\n",
    "print(\"\\nTop 20 co-occurring hashtags:\")\n",
    "print(top_20_hashtags)\n",
    "\n",
    "# Hashtag co-occurrence\n",
    "def get_hashtag_pairs(hashtags):\n",
    "    return [(a, b) for idx, a in enumerate(hashtags) for b in hashtags[idx + 1:]]\n",
    "\n",
    "hashtag_pairs = data['Comment'].apply(lambda x: get_hashtag_pairs(extract_hashtags(x))).sum()\n",
    "pair_counts = Counter(hashtag_pairs)\n",
    "\n",
    "print(\"\\nTop 10 co-occurring hashtag pairs:\")\n",
    "for pair, count in pair_counts.most_common(10):\n",
    "    print(f\"{pair}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last four rows in the output represent co-occurring hashtag pairs. These are pairs of hashtags that appear together in the same tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwcjpQMv3zdZ"
   },
   "source": [
    "# Sentiment Analysis (Bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BERT-specific preprocessing\n",
    "def bert_preprocess(text):\n",
    "    \"\"\"Minimal preprocessing optimized for BERT models\"\"\"\n",
    "    text = str(text) if pd.notnull(text) else \"\"\n",
    "    text = text.replace('\\n', ' ')    # Remove newlines\n",
    "    text = text.strip()               # Remove leading/trailing whitespace\n",
    "    return text[:512]                 # Truncate to BERT's max length\n",
    "\n",
    "# Using Twitter-optimized BERT model\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"  # for English tweets\n",
    "\n",
    "# Initializing components with error handling\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configuring pipeline with batch processing\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  \n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    batch_size=16 \n",
    ")\n",
    "\n",
    "def analyze_tweets(texts):\n",
    "    \"\"\"Process tweets in batches with error handling\"\"\"\n",
    "    results = []\n",
    "    for batch in tqdm([texts[i:i+16] for i in range(0, len(texts), 16)]):\n",
    "        try:\n",
    "            batch_results = sentiment_analyzer(batch)\n",
    "            results.extend([result['label'] for result in batch_results])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            results.extend(['neutral']*len(batch))  # Fallback for failed batches\n",
    "    return results\n",
    "\n",
    "# Preprocessing and analyzing\n",
    "data['clean_text_bert'] = data['Comment'].apply(bert_preprocess)\n",
    "valid_texts = data[data['clean_text_bert'] != '']['clean_text_bert'].tolist()\n",
    "\n",
    "print(f\"Analyzing {len(valid_texts)} tweets...\")\n",
    "sentiments = analyze_tweets(valid_texts)\n",
    "\n",
    "# Mapping predictions to standard labels\n",
    "label_map = {\n",
    "    'LABEL_0': 'negative',\n",
    "    'LABEL_1': 'neutral',\n",
    "    'LABEL_2': 'positive'\n",
    "}\n",
    "\n",
    "data['sentiment'] = 'neutral'  # Default value\n",
    "data.loc[data['clean_text_bert'] != '', 'sentiment'] = [label_map.get(s, 'neutral') for s in sentiments]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of sentiment based on superapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "\n",
    "# Getting unique categories\n",
    "categories = sorted(data['category'].unique())\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adding traces for each category\n",
    "for category in categories:\n",
    "    filtered_data = data[data['category'] == category]\n",
    "    sentiment_counts = filtered_data['sentiment'].value_counts()\n",
    "\n",
    "    # Calculating percentage\n",
    "    total_comments = sentiment_counts.sum()\n",
    "    sentiment_percentages = (sentiment_counts / total_comments) * 100\n",
    "\n",
    "    # Adding trace with percentage labels\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sentiment_counts.index,\n",
    "        y=sentiment_counts.values,\n",
    "        name=str(category),\n",
    "        visible=False,\n",
    "        text=[f'{p:.2f}%' for p in sentiment_percentages],\n",
    "        textposition='inside',\n",
    "        hovertext=[f'{count} comments\\n({p:.2f}%)' for count, p in zip(sentiment_counts, sentiment_percentages)],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "# Adding trace for all categories\n",
    "all_sentiment_counts = data['sentiment'].value_counts()\n",
    "total_all_comments = all_sentiment_counts.sum()\n",
    "all_sentiment_percentages = (all_sentiment_counts / total_all_comments) * 100\n",
    "fig.add_trace(go.Bar(\n",
    "    x=all_sentiment_counts.index,\n",
    "    y=all_sentiment_counts.values,\n",
    "    name=\"All Categories\",\n",
    "    visible=True,\n",
    "    text=[f'{p:.2f}%' for p in all_sentiment_percentages],\n",
    "    textposition='inside',\n",
    "    hovertext=[f'{count} comments\\n({p:.2f}%)' for count, p in zip(all_sentiment_counts, all_sentiment_percentages)],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "buttons = []\n",
    "\n",
    "# Adding a button for each category\n",
    "for i, category in enumerate(categories):\n",
    "    button = dict(\n",
    "        label=str(category),\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": [j == i for j in range(len(categories))] + [False]},\n",
    "            {\"title\": f\"Sentiment Distribution (Category: {category})\"}\n",
    "        ]\n",
    "    )\n",
    "    buttons.append(button)\n",
    "\n",
    "# Adding button to show all categories\n",
    "buttons.append(\n",
    "    dict(\n",
    "        label=\"All Categories\",\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": [False] * len(categories) + [True]},\n",
    "            {\"title\": \"Sentiment Distribution (All Categories)\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Updating layout with buttons\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(\n",
    "        type=\"buttons\",\n",
    "        buttons=buttons,\n",
    "        direction=\"down\",\n",
    "        showactive=True,\n",
    "        x=1.1,\n",
    "        xanchor=\"right\",\n",
    "        y=1.15,\n",
    "        yanchor=\"top\"\n",
    "    )],\n",
    "    title=\"Sentiment Distribution (All Categories)\",\n",
    "    xaxis_title=\"Sentiment\",\n",
    "    yaxis_title=\"Number of Comments\",\n",
    "    barmode=\"group\"\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of sentiment based on the platform (YouTube or Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "\n",
    "# Getting unique categories\n",
    "sources = sorted(data['source'].unique())\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adding traces for each category\n",
    "for source in sources:\n",
    "    filtered_data = data[data['source'] == source]\n",
    "    sentiment_counts = filtered_data['sentiment'].value_counts()\n",
    "\n",
    "    # Calculating percentage\n",
    "    total_comments = sentiment_counts.sum()\n",
    "    sentiment_percentages = (sentiment_counts / total_comments) * 100\n",
    "\n",
    "    # Adding trace with percentage labels\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=sentiment_counts.index,\n",
    "        y=sentiment_counts.values,\n",
    "        name=str(source),\n",
    "        visible=False,\n",
    "        text=[f'{p:.2f}%' for p in sentiment_percentages],\n",
    "        textposition='inside',\n",
    "        hovertext=[f'{count} comments\\n({p:.2f}%)' for count, p in zip(sentiment_counts, sentiment_percentages)],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "# Adding a trace for all categories\n",
    "all_sentiment_counts = data['sentiment'].value_counts()\n",
    "total_all_comments = all_sentiment_counts.sum()\n",
    "all_sentiment_percentages = (all_sentiment_counts / total_all_comments) * 100\n",
    "fig.add_trace(go.Bar(\n",
    "    x=all_sentiment_counts.index,\n",
    "    y=all_sentiment_counts.values,\n",
    "    name=\"All Categories\",\n",
    "    visible=True,\n",
    "    text=[f'{p:.2f}%' for p in all_sentiment_percentages],\n",
    "    textposition='inside',\n",
    "    hovertext=[f'{count} comments\\n({p:.2f}%)' for count, p in zip(all_sentiment_counts, all_sentiment_percentages)],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "buttons = []\n",
    "\n",
    "# Adding a button for each category\n",
    "for i, source in enumerate(sources):\n",
    "    button = dict(\n",
    "        label=str(source),\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": [j == i for j in range(len(sources))] + [False]},\n",
    "            {\"title\": f\"Sentiment Distribution (Category: {source})\"}\n",
    "        ]\n",
    "    )\n",
    "    buttons.append(button)\n",
    "\n",
    "# Adding button to show all categories\n",
    "buttons.append(\n",
    "    dict(\n",
    "        label=\"All Sources\",\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": [False] * len(sources) + [True]},\n",
    "            {\"title\": \"Sentiment Distribution (All Categories)\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Updating layout with buttons\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(\n",
    "        type=\"buttons\",\n",
    "        buttons=buttons,\n",
    "        direction=\"down\",\n",
    "        showactive=True,\n",
    "        x=1.1,\n",
    "        xanchor=\"right\",\n",
    "        y=1.15,\n",
    "        yanchor=\"top\"\n",
    "    )],\n",
    "    title=\"Sentiment Distribution (All Sources)\",\n",
    "    xaxis_title=\"Sentiment\",\n",
    "    yaxis_title=\"Number of Comments\",\n",
    "    barmode=\"group\"\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Frequency Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def sentiment_breakdown(df, granularity='monthly'):\n",
    "    # Color configuration\n",
    "    color_map = {\n",
    "        'positive': '#4CAF50',  # Green\n",
    "        'neutral': '#2196F3',   # Blue\n",
    "        'negative': '#F44336'   # Red\n",
    "    }\n",
    "    \n",
    "    # Verify Date_of_comment exists\n",
    "    if 'Date_of_comment' not in df.columns:\n",
    "        if 'Date_of_comment' in df.index.names:\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            raise ValueError(\"Date_of_comment column not found in DataFrame\")\n",
    "\n",
    "    breakdown = df['sentiment'].value_counts(normalize=True) * 100\n",
    "    print(f\"\\nSentiment Distribution:\\n{breakdown}\")\n",
    "    \n",
    "    # Enhanced temporal analysis\n",
    "    df['date'] = pd.to_datetime(df['Date_of_comment'])\n",
    "    \n",
    "    if granularity == 'weekly':\n",
    "        df['date'] = df['date'].dt.to_period('W').dt.start_time\n",
    "    elif granularity == 'monthly':\n",
    "        df['date'] = df['date'].dt.to_period('M').dt.start_time\n",
    "    else:  # default to daily\n",
    "        df['date'] = df['date'].dt.floor('D')\n",
    "    \n",
    "    # Sentiment over time\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        subset = df[df['sentiment'] == sentiment]\n",
    "        counts = subset.groupby('date').size()\n",
    "        if granularity == 'daily':\n",
    "            counts.rolling(7).mean().plot(label=sentiment, alpha=0.8, color=color_map[sentiment])\n",
    "        elif granularity == 'weekly':\n",
    "            counts.rolling(4).mean().plot(label=sentiment, alpha=0.8, color=color_map[sentiment])\n",
    "        elif granularity == 'monthly':\n",
    "            counts.plot(label=sentiment, alpha=0.8, color=color_map[sentiment])  # Direct plot for monthly data\n",
    "    \n",
    "    plt.title(f'Sentiment Trends ({granularity.capitalize()} Analysis)')\n",
    "    plt.ylabel(f'Comment Count ({granularity})')  # Updated label for monthly data\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Sentiment by hour of day\n",
    "    df['hour'] = pd.to_datetime(df['Date_of_comment']).dt.hour\n",
    "    hourly_sentiment = df.groupby(['hour', 'sentiment']).size().unstack()\n",
    "    \n",
    "    hourly_sentiment = hourly_sentiment[['positive', 'neutral', 'negative']]\n",
    "    \n",
    "    hourly_sentiment.plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        figsize=(12,6),\n",
    "        color=[color_map[col] for col in hourly_sentiment.columns]\n",
    "    )\n",
    "    plt.title('Sentiment Distribution by Hour of Day')\n",
    "    plt.xlabel('Hour (UTC)')\n",
    "    plt.ylabel('Number of comments')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data['Date_of_comment'] = pd.to_datetime(data['Date_of_comment'])\n",
    "sentiment_breakdown(data.copy(), granularity='monthly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds - Most Frequent Words – Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for the whole dataset\n",
    "# Word frequency\n",
    "word_freq = Counter()\n",
    "for post in data['tokens']:\n",
    "    word_freq.update(post)\n",
    "\n",
    "# Generating the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Displaying the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words Overall')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds - Most Frequent Words – with few exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining words to exclude\n",
    "exclude_words = {\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \"via\", \"us\", \n",
    "\"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"platform\"}\n",
    "\n",
    "# Word frequency\n",
    "word_freq_1 = Counter()\n",
    "for post in data['tokens']:\n",
    "    filtered_tokens = [word for word in post if word.lower() not in exclude_words]\n",
    "    word_freq_1.update(filtered_tokens)\n",
    "\n",
    "# Generating the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq_1)\n",
    "# Displaying the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.title('Most Frequent Words with few exclusions')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table – Top 20 most used words – Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most frequent words\n",
    "# Sorting word frequencies in descending order and get the top 20\n",
    "top_20_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# Generating the order with correct suffixes\n",
    "def get_ordinal_suffix(n):\n",
    "    if 10 <= n % 100 <= 20: \n",
    "        return f'{n}th'\n",
    "    else:\n",
    "        suffixes = {1: 'st', 2: 'nd', 3: 'rd'}\n",
    "        return f'{n}{suffixes.get(n % 10, \"th\")}'\n",
    "\n",
    "# Generating the \"Place\" row with suffixes\n",
    "order_with_suffix = [get_ordinal_suffix(i + 1) for i in range(len(top_20_word_freq))]\n",
    "\n",
    "# Preparing the table data\n",
    "table_data = [['Place', *order_with_suffix]]  # First row with place labels\n",
    "table_data.append(['Word'] + [word for word, freq in top_20_word_freq])  # Second row with words\n",
    "table_data.append(['Frequency'] + [freq for word, freq in top_20_word_freq])  # Third row with frequencies\n",
    "\n",
    "# Printing the transposed table\n",
    "print('Top 20 Most Common Words (Overall):')\n",
    "print(tabulate(table_data, headers='firstrow', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table – Top 20 most used words – with few exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 with excluded words\n",
    "# Words to exclude from the table\n",
    "exclude_words = {\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \n",
    "\"via\", \"us\", \"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"would\", \"know\", \"think\", \"want\", \"say\", \"see\", \"even\"}\n",
    "# Sorting the original word frequencies by frequency in descending order\n",
    "sorted_word_freq = sorted(word_freq_1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Filtering out the words to exclude\n",
    "filtered_sorted_word_freq = [(word, freq) for word, freq in sorted_word_freq if word not in exclude_words]\n",
    "\n",
    "# Taking the top 20 after filtering\n",
    "top_20_filtered_word_freq = filtered_sorted_word_freq[:20]\n",
    "\n",
    "# Generating the order with correct suffixes for the top 20\n",
    "order_with_suffix_filtered = [get_ordinal_suffix(i + 1) for i in range(len(top_20_filtered_word_freq))]\n",
    "\n",
    "# Preparing the table data\n",
    "filtered_table_data = [['Place', *order_with_suffix_filtered]]  # First row with place labels\n",
    "filtered_table_data.append(['Word'] + [word for word, freq in top_20_filtered_word_freq])  # Second row with words\n",
    "filtered_table_data.append(['Frequency'] + [freq for word, freq in top_20_filtered_word_freq])  # Third row with frequencies\n",
    "\n",
    "# Printing the filtered table\n",
    "print('Top 20 Most Common Words (Filtered):')\n",
    "print(tabulate(filtered_table_data, headers='firstrow', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds - Most Frequent Words related to Wechat, AliPay and KakaoTalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "# List of words to exclude (stop words)\n",
    "stop_words = [\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \"via\", \"us\", \"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"would\", \"know\", \"think\", \"want\", \"say\", \"see\", \"even\"]\n",
    "\n",
    "# Combining all comments for each party and preprocess text\n",
    "party_comments = {}\n",
    "for category, party in zip([\"wechat\", \"kakaotalk\", \"alipay\"], [\"WeChat\", \"KakaoTalk\", \"AliPay\"]):\n",
    "    comments = ' '.join([' '.join(map(str, tokens)) for tokens in data[data['category'] == category]['tokens']])\n",
    "    \n",
    "    # Converting to lowercase and remove punctuation\n",
    "    comments = comments.lower()\n",
    "    comments = re.sub(r'[^\\w\\s]', '', comments)\n",
    "    \n",
    "    # Removing stop words\n",
    "    words = comments.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_comments = ' '.join(filtered_words)\n",
    "    \n",
    "    party_comments[party] = filtered_comments\n",
    "\n",
    "# Generating word clouds\n",
    "wordclouds = {}\n",
    "for party, comments in party_comments.items():\n",
    "    wordclouds[party] = WordCloud(width=800, height=400, background_color='white').generate(comments)\n",
    "\n",
    "# Plotting the word clouds\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, (party, wordcloud) in enumerate(wordclouds.items(), 1):\n",
    "    plt.subplot(1, len(wordclouds), i)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Most Common Words Associated with {party} platform')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables - Most Frequent Words related to Wechat, AliPay and KakaoTalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Function to get ordinal suffix (e.g., 1st, 2nd, 3rd)\n",
    "def get_ordinal_suffix(n):\n",
    "    if 11 <= n % 100 <= 13:\n",
    "        return f\"{n}th\"\n",
    "    else:\n",
    "        suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "        return f\"{n}{suffix}\"\n",
    "\n",
    "# List of words to exclude (stop words)\n",
    "stop_words = {\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \"via\", \"us\", \"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"would\", \"know\", \"think\", \"want\", \"say\", \"see\", \"even\"}\n",
    "\n",
    "# Processing each category and generating tables\n",
    "for category in [\"wechat\", \"kakaotalk\", \"alipay\"]:\n",
    "    # Filtering data for the current category\n",
    "    category_data = data[data['category'] == category]\n",
    "    \n",
    "    # Combining all tokens for the category\n",
    "    all_tokens = [token for tokens_list in category_data['tokens'] for token in tokens_list]\n",
    "    \n",
    "    # Filtering out stop words\n",
    "    filtered_tokens = [token for token in all_tokens if token not in stop_words]\n",
    "    \n",
    "    # Calculating word frequencies\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)  # Sorting by frequency\n",
    "    \n",
    "    # Taking the top 20 words\n",
    "    top_20_word_freq = sorted_word_freq[:20]\n",
    "    \n",
    "    # Generating the order with correct suffixes for the top 20\n",
    "    order_with_suffix = [get_ordinal_suffix(i + 1) for i in range(len(top_20_word_freq))]\n",
    "    \n",
    "    # Preparing the table data\n",
    "    table_data = [['Place', *order_with_suffix]]  # First row with place labels\n",
    "    table_data.append(['Word'] + [word for word, freq in top_20_word_freq])  # Second row with words\n",
    "    table_data.append(['Frequency'] + [freq for word, freq in top_20_word_freq])  # Third row with frequencies\n",
    "    \n",
    "    # Printing the table\n",
    "    print(f'\\nTop 20 Most Common Words for {category.capitalize()}:')\n",
    "    print(tabulate(table_data, headers='firstrow', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds - Most Frequent Words related to Revolut, Grab and Gojek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "# List of words to exclude (stop words)\n",
    "stop_words = [\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \"via\", \"us\", \"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"would\", \"know\", \"think\", \"want\", \"say\", \"see\", \"even\"]\n",
    "\n",
    "# Combining all comments for each party and preprocessing text\n",
    "party_comments = {}\n",
    "for category, party in zip([\"revolut\", \"grab\", \"gojek\"], [\"Revolut\", \"Grab\", \"Gojek\"]):\n",
    "    comments = ' '.join([' '.join(map(str, tokens)) for tokens in data[data['category'] == category]['tokens']])\n",
    "    \n",
    "    # Converting to lowercase and removing punctuation\n",
    "    comments = comments.lower()\n",
    "    comments = re.sub(r'[^\\w\\s]', '', comments)\n",
    "    \n",
    "    # Removing stop words\n",
    "    words = comments.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    filtered_comments = ' '.join(filtered_words)\n",
    "    \n",
    "    party_comments[party] = filtered_comments\n",
    "\n",
    "# Generating word clouds\n",
    "wordclouds = {}\n",
    "for party, comments in party_comments.items():\n",
    "    wordclouds[party] = WordCloud(width=800, height=400, background_color='white').generate(comments)\n",
    "\n",
    "# Plotting the word clouds\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, (party, wordcloud) in enumerate(wordclouds.items(), 1):\n",
    "    plt.subplot(1, len(wordclouds), i)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Most Common Words Associated with {party} platform')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables - Most Frequent Words related to Revolut, Grab and Gojek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Function to get ordinal suffix (e.g., 1st, 2nd, 3rd)\n",
    "def get_ordinal_suffix(n):\n",
    "    if 11 <= n % 100 <= 13:\n",
    "        return f\"{n}th\"\n",
    "    else:\n",
    "        suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "        return f\"{n}{suffix}\"\n",
    "\n",
    "# List of words to exclude (stop words)\n",
    "stop_words = {\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \"via\", \"us\", \"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"would\", \"know\", \"think\", \"want\", \"say\", \"see\", \"even\"}\n",
    "\n",
    "# Processing each category and generating tables\n",
    "for category in [\"grab\", \"gojek\", \"revolut\"]:\n",
    "    # Filtering data for the current category\n",
    "    category_data = data[data['category'] == category]\n",
    "    \n",
    "    # Combining all tokens for the category\n",
    "    all_tokens = [token for tokens_list in category_data['tokens'] for token in tokens_list]\n",
    "    \n",
    "    # Filtering out stop words\n",
    "    filtered_tokens = [token for token in all_tokens if token not in stop_words]\n",
    "    \n",
    "    # Calculating word frequencies\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)  # Sort by frequency\n",
    "    \n",
    "    # Taking the top 20 words\n",
    "    top_20_word_freq = sorted_word_freq[:20]\n",
    "    \n",
    "    # Generating the order with correct suffixes for the top 20\n",
    "    order_with_suffix = [get_ordinal_suffix(i + 1) for i in range(len(top_20_word_freq))]\n",
    "    \n",
    "    # Preparing the table data\n",
    "    table_data = [['Place', *order_with_suffix]]  # First row with place labels\n",
    "    table_data.append(['Word'] + [word for word, freq in top_20_word_freq])  # Second row with words\n",
    "    table_data.append(['Frequency'] + [freq for word, freq in top_20_word_freq])  # Third row with frequencies\n",
    "    \n",
    "    # Printing the table\n",
    "    print(f'\\nTop 20 Most Common Words for {category.capitalize()}:')\n",
    "    print(tabulate(table_data, headers='firstrow', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables - Most Frequent Words on YouTube and Twitter (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_youtube = data[data['source'].str.contains('super_apps_youtube', case=False, na=False)]\n",
    "subset_data_twitter = data[data['source'].str.contains('super_apps_youtube', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Function to get ordinal suffix (e.g., 1st, 2nd, 3rd)\n",
    "def get_ordinal_suffix(n):\n",
    "    if 11 <= n % 100 <= 13:\n",
    "        return f\"{n}th\"\n",
    "    else:\n",
    "        suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
    "        return f\"{n}{suffix}\"\n",
    "\n",
    "# List of words to exclude (stop words)\n",
    "stop_words = {\"super\", \"app\", \"superapp\", \"super app\", \"super-app\", \"super apps\", \"superapps\", \"apps\", \"super-apps\", \"via\", \"us\", \"get\", \"make\", \"use\", \"like\", \"new\", \"need\", \"go\", \"create\", \"become\", \"take\", \"would\", \"know\", \"think\", \"want\", \"say\", \"see\", \"even\"}\n",
    "\n",
    "# Processing each category and generate tables\n",
    "for source in [\"super_apps_youtube\", \"super_apps_twitter\"]:\n",
    "    # Filtering data for the current category\n",
    "    source_data = data[data['source'] == source]\n",
    "    \n",
    "    # Combining all tokens for the category\n",
    "    all_tokens = [token for tokens_list in source_data['tokens'] for token in tokens_list]\n",
    "    \n",
    "    # Filtering out stop words\n",
    "    filtered_tokens = [token for token in all_tokens if token not in stop_words]\n",
    "    \n",
    "    # Calculating word frequencies\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)  # Sorting by frequency\n",
    "    \n",
    "    # Taking the top 20 words\n",
    "    top_20_word_freq = sorted_word_freq[:20]\n",
    "    \n",
    "    # Generating the order with correct suffixes for the top 20\n",
    "    order_with_suffix = [get_ordinal_suffix(i + 1) for i in range(len(top_20_word_freq))]\n",
    "    \n",
    "    # Preparing the table data\n",
    "    table_data = [['Place', *order_with_suffix]]  # First row with place labels\n",
    "    table_data.append(['Word'] + [word for word, freq in top_20_word_freq])  # Second row with words\n",
    "    table_data.append(['Frequency'] + [freq for word, freq in top_20_word_freq])  # Third row with frequencies\n",
    "    \n",
    "    # Printing the table\n",
    "    print(f'\\nTop 20 Most Common Words for {source.capitalize()}:')\n",
    "    print(tabulate(table_data, headers='firstrow', tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the most frequent bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Defining words to omit\n",
    "omit_words = {\"super\", \"app\", \"apps\", \"in\", \"of\", \"is\", \"on\", \"with\", \"for\", \"at\"} \n",
    "\n",
    "# Combining all tokens into a single string\n",
    "text = ' '.join([' '.join(tokens) for tokens in data['tokens']])\n",
    "\n",
    "# Filtering out unwanted words\n",
    "filtered_tokens = [word for word in text.split() if word.lower() not in omit_words]\n",
    "\n",
    "# Generating bigrams\n",
    "n = 2\n",
    "grams = ngrams(filtered_tokens, n)\n",
    "gram_counts = Counter(grams)\n",
    "\n",
    "# Getting the most common n-grams\n",
    "top_n_grams = gram_counts.most_common(20)\n",
    "\n",
    "# Preparing data for plotting\n",
    "labels, values = zip(*top_n_grams)\n",
    "labels = [' '.join(gram) for gram in labels]\n",
    "\n",
    "# Plotting bigrams\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('N-grams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Top 20 Most Frequent {n}-grams (Filtered)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Displaying as table using pandas\n",
    "bigram_table = pd.DataFrame(top_n_grams, columns=['Bigram', 'Frequency'])\n",
    "bigram_table['Bigram'] = bigram_table['Bigram'].apply(lambda x: ' '.join(x))  # Converting tuple to string\n",
    "print(\"\\nTop 20 Most Frequent Bigrams:\\n\")\n",
    "print(bigram_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling – YouTube data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_youtube = data[data['source'].str.contains('super_apps_youtube', case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(subset_data_youtube))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization – YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook_connected'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "extra_stopwords = ['the', 'to', 'and', 'is', 'of', 'in', 'for', 'app', 'apps', 'platform', 'superapp', 'superapps' ,'super','it', 'on', 'this', 'that', 'you', 'your', 'with', 'are', 'we']\n",
    "all_stop_words = nltk_stopwords.union(set(ENGLISH_STOP_WORDS))\n",
    "\n",
    "# Data Cleaning Function\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in all_stop_words]) \n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Loading and Cleaning Data\n",
    "subset_data_youtube['cleaned_body'] = subset_data_youtube['Comment'].apply(clean_text)\n",
    "\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=list(all_stop_words))\n",
    "\n",
    "# Model Configuration\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=8,  \n",
    "    min_dist=0.1,   \n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50, \n",
    "    min_samples=15,        \n",
    "    cluster_selection_epsilon=0.1,\n",
    "    prediction_data=True,\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "\n",
    "\n",
    "# BERTopic Model Initialization and Fitting\n",
    "topic_model_youtube = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    min_topic_size=150,\n",
    "    nr_topics=\"auto\",\n",
    "    low_memory=True,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fitting the model\n",
    "topics, probs = topic_model_youtube.fit_transform(subset_data_youtube['cleaned_body'].tolist())\n",
    "\n",
    "\n",
    "\n",
    "# Additional outlier reduction\n",
    "if -1 in topics:\n",
    "    print(\"Reducing outliers...\")\n",
    "    new_topics = topic_model_youtube.reduce_outliers(subset_data_youtube['cleaned_body'].tolist(), topics, strategy=\"c-tf-idf\", threshold=0.05)\n",
    "    new_topics = topic_model_youtube.reduce_outliers(subset_data_youtube['cleaned_body'].tolist(), new_topics, strategy=\"distributions\", threshold=0.02)\n",
    "    topic_model_youtube.update_topics(subset_data_youtube['cleaned_body'].tolist(), topics=new_topics)\n",
    "\n",
    "\n",
    "\n",
    "# Visualizations\n",
    "topic_model_youtube.visualize_topics(width=1000, height=600).show()\n",
    "topic_model_youtube.visualize_barchart(top_n_topics=10).show()\n",
    "\n",
    "topic_info_youtube = topic_model_youtube.get_topic_info()\n",
    "print(topic_info_youtube)\n",
    "\n",
    "# Printing representative docs for top 5 topics\n",
    "for topic in topic_info_youtube['Topic'][:5]:\n",
    "    if topic != -1:\n",
    "        print(f\"\\nTopic {topic} Representative Documents:\")\n",
    "        rep_docs = topic_model_youtube.get_representative_docs(topic)\n",
    "        for doc in rep_docs[:3]:\n",
    "            print(doc)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting detailed information about gotten topics – YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "# Getting topic information\n",
    "topic_info_youtube = topic_model_youtube.get_topic_info()\n",
    "\n",
    "# Getting more details for each topic\n",
    "topic_details_youtube = []\n",
    "for topic in topic_info_youtube['Topic']:\n",
    "    if topic != -1:  # Excluding the outlier topic\n",
    "        words = topic_model_youtube.get_topic(topic)\n",
    "        docs = topic_model_youtube.get_representative_docs(topic)\n",
    "        topic_details_youtube.append({\n",
    "            'Topic': topic,\n",
    "            'Name': topic_info_youtube[topic_info_youtube['Topic'] == topic]['Name'].values[0],\n",
    "            'Count': topic_info_youtube[topic_info_youtube['Topic'] == topic]['Count'].values[0],\n",
    "            'Top Words': ', '.join([word for word, _ in words[:10]]),\n",
    "            'Representative Docs': docs[:3]  # Getting the first 3 representative documents\n",
    "        })\n",
    "\n",
    "# Converting to DataFrame for easier viewing\n",
    "df_topic_details_youtube = pd.DataFrame(topic_details_youtube)\n",
    "print(df_topic_details_youtube.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling – Twitter (X) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data_twitter = data[data['source'].str.contains('super_apps_twitter', case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(subset_data_twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization – Twitter (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook_connected'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "import re\n",
    "\n",
    "# Data Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Loading and Cleaning Data\n",
    "subset_data_twitter['cleaned_body'] = subset_data_twitter['Comment'].apply(clean_text)\n",
    "\n",
    "# Custom Stopwords\n",
    "custom_stopwords = [\n",
    "    \"superapp\", \"apps\", \"app\", \"services\", \"platform\", \"digital\", \"social\", \"thank\", \"good\", \"like\", \"south\", \n",
    "    \"just\", \"hey\", \"oh\", \"really\", \"little\", \"im\", \"dont\", \"youre\", \"hes\", \"shes\", \n",
    "    \"theyre\", \"its\", \"super\", \"one\", \"com\"\n",
    "]\n",
    "all_stop_words = set(ENGLISH_STOP_WORDS)#.union(set(custom_stopwords))\n",
    "\n",
    "# Model Configuration\n",
    "vectorizer_model = CountVectorizer(stop_words=list(all_stop_words))\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=10,\n",
    "    min_dist=0.05,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=15,\n",
    "    cluster_selection_epsilon=0.1,\n",
    "    prediction_data=True,\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# BERTopic Model Initialization and Fitting\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    min_topic_size=100, \n",
    "    nr_topics=\"auto\",\n",
    "    low_memory=True,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fitting the model\n",
    "topics, probs = topic_model.fit_transform(subset_data_twitter['cleaned_body'].tolist())\n",
    "\n",
    "# Additional outlier reduction\n",
    "if -1 in topics:\n",
    "    print(\"Reducing outliers...\")\n",
    "    new_topics = topic_model.reduce_outliers(subset_data_twitter['cleaned_body'].tolist(), topics, strategy=\"c-tf-idf\", threshold=0.1)\n",
    "    new_topics = topic_model.reduce_outliers(subset_data_twitter['cleaned_body'].tolist(), new_topics, strategy=\"distributions\", threshold=0.01)\n",
    "    topic_model.update_topics(subset_data_twitter['cleaned_body'].tolist(), topics=new_topics)\n",
    "\n",
    "\n",
    "\n",
    "# Visualizations\n",
    "topic_model.visualize_topics(width=1000, height=600).show()\n",
    "topic_model.visualize_barchart(top_n_topics=10).show()\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "\n",
    "# Printing representative docs for top 5 topics\n",
    "for topic in topic_info['Topic'][:5]:\n",
    "    if topic != -1:\n",
    "        print(f\"\\nTopic {topic} Representative Documents:\")\n",
    "        rep_docs = topic_model.get_representative_docs(topic)\n",
    "        for doc in rep_docs[:3]:\n",
    "            print(doc)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting detailed information about gotten topics – Twitter (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "\n",
    "# Getting topic information\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Getting more details for each topic\n",
    "topic_details = []\n",
    "for topic in topic_info['Topic']:\n",
    "    if topic != -1:  # Excluding the outlier topic\n",
    "        words = topic_model.get_topic(topic)\n",
    "        docs = topic_model.get_representative_docs(topic)\n",
    "        topic_details.append({\n",
    "            'Topic': topic,\n",
    "            'Name': topic_info[topic_info['Topic'] == topic]['Name'].values[0],\n",
    "            'Count': topic_info[topic_info['Topic'] == topic]['Count'].values[0],\n",
    "            'Top Words': ', '.join([word for word, _ in words[:10]]),\n",
    "            'Representative Docs': docs[:3]  # Getting the first 3 representative documents\n",
    "        })\n",
    "\n",
    "# Converting to DataFrame for easier viewing\n",
    "df_topic_details = pd.DataFrame(topic_details)\n",
    "print(df_topic_details.to_string())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
